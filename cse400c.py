# app.py
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import gdown
import os

# ---------------------------
# Page title
# ---------------------------
st.title("Bangla Newspaper Dataset Analysis")

# ---------------------------
# Download dataset if not exists
# ---------------------------
file_id = "1KYEuvvLLV7a0IRTaOU-U5X9Ysf6wN7W8"
output = "newspaper.json"
url = f"https://drive.google.com/uc?id={file_id}"

if not os.path.exists(output):
    st.write("Downloading dataset...")
    gdown.download(url, output, quiet=False)
else:
    st.write("Dataset already exists.")

# ---------------------------
# Load dataset
# ---------------------------
df1 = pd.read_json(output)

# ---------------------------
# Clean dataset
# ---------------------------
df1 = df1.drop_duplicates(subset='content', keep='first')
df1 = df1[~df1['category'].isin(['bangladesh', 'opinion'])]
df1 = df1.reset_index(drop=True)

# ---------------------------
# Show dataset info
# ---------------------------
st.subheader("Cleaned Dataset Info")
st.text(df1.info())
st.subheader("Sample Data")
st.dataframe(df1.head())

# ---------------------------
# Plot category counts
# ---------------------------
category_counts = df1['category'].value_counts()
fig, ax = plt.subplots(figsize=(12,6))
category_counts.plot(kind='bar', color='skyblue', ax=ax)
ax.set_title("Number of Articles per Category (Cleaned Dataset)", fontsize=16)
ax.set_xlabel("Category", fontsize=14)
ax.set_ylabel("Number of Articles", fontsize=14)
plt.xticks(rotation=45)

st.subheader("Category Distribution")
st.pyplot(fig)

# app_dataset2.py
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import gdown
import os

# ---------------------------
# Page title
# ---------------------------
st.title("Bangla Newspaper Dataset 2 Analysis")

# ---------------------------
# Download dataset if not exists
# ---------------------------
file_id = "1OtPy0n-LsceeDPJI5yfK8ekVneHHkeLR"
output = "Bangla_Newspaper_Article_Dataset.csv"
url = f"https://drive.google.com/uc?id={file_id}"

if not os.path.exists(output):
    st.write("Downloading Dataset 2 (this may take a while)...")
    gdown.download(url, output, quiet=False)
else:
    st.write("Dataset 2 already exists locally. Skipping download.")

# ---------------------------
# Load dataset
# ---------------------------
df2 = pd.read_csv(output, encoding='utf-8')

st.subheader("Dataset Info")
st.text(df2.info())
st.subheader("Sample Data")
st.dataframe(df2.head())

# ---------------------------
# Plot category distribution
# ---------------------------
category_counts2 = df2['category'].value_counts()

fig, ax = plt.subplots(figsize=(10, 6))
sns.barplot(x=category_counts2.index, y=category_counts2.values, palette='magma', ax=ax)

# Add frequency labels on top of bars
for container in ax.containers:
    ax.bar_label(container, fmt='%d', label_type='edge')

ax.set_title('Dataset 2 News Category Distribution')
ax.set_xlabel('Category')
ax.set_ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()

st.subheader("Category Distribution")
st.pyplot(fig)
# app_data_processing.py
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ---------------------------
# Page title
# ---------------------------
st.title("Bangla News Data Processing & Visualization")

# ---------------------------
# Assume df1 and df2 are already loaded (from previous apps)
# ---------------------------
# df1 = ... (Dataset 1)
# df2 = ... (Dataset 2)

st.subheader("Initial Dataset Info")
st.write("Dataset 1 info:")
st.text(df1.info())
st.dataframe(df1.head())

st.write("Dataset 2 info:")
st.text(df2.info())
st.dataframe(df2.head())

# ---------------------------
# Data Cleaning
# ---------------------------
df1_clean = df1.drop(columns=['author', 'category_bn', 'modification_date', 'tag', 'comment_count'], errors='ignore')
df1_clean.rename(columns={'url': 'source'}, inplace=True)
df1_clean = df1_clean.dropna().reset_index(drop=True)
df1_clean = df1_clean.drop_duplicates(subset=['content'], keep='first').reset_index(drop=True)
df1_clean['category'] = df1_clean['category'].replace('life-style', 'lifestyle')
df1_clean = df1_clean[~df1_clean['category'].isin(['bangladesh', 'opinion'])]

df2_clean = df2.dropna().reset_index(drop=True)
df2_clean = df2_clean.drop_duplicates(subset=['content'], keep='first').reset_index(drop=True)

# ---------------------------
# Combine datasets
# ---------------------------
df = pd.concat([df1_clean, df2_clean], ignore_index=True)
st.subheader("Combined Dataset Info")
st.text(df.info())
st.write("Category counts before balancing:")
st.write(df['category'].value_counts())

# ---------------------------
# Plot histogram of categories
# ---------------------------
fig, ax = plt.subplots(figsize=(10, 6))
category_counts = df['category'].value_counts()
sns.barplot(x=category_counts.index, y=category_counts.values, palette='magma', ax=ax)
ax.set_title('News Category Distribution (Before Balancing)')
ax.set_xlabel('Category')
ax.set_ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
st.pyplot(fig)

# ---------------------------
# Balance classes
# ---------------------------
target_size = 5000
balanced_dfs = []

for category, group in df.groupby('category'):
    if len(group) > target_size:
        sampled = group.iloc[-target_size:]  # keep last rows
    else:
        sampled = group
    balanced_dfs.append(sampled)

df_balanced = pd.concat(balanced_dfs, ignore_index=True)
df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

st.subheader("Balanced Dataset Info")
st.text(df_balanced.info())
st.write("Category counts after balancing:")
st.write(df_balanced['category'].value_counts())
st.write("Shape:", df_balanced.shape)

# ---------------------------
# Plot histogram after balancing
# ---------------------------
fig2, ax2 = plt.subplots(figsize=(10, 6))
category_counts_bal = df_balanced['category'].value_counts()
sns.barplot(x=category_counts_bal.index, y=category_counts_bal.values, palette='magma', ax=ax2)
ax2.set_title('News Category Distribution (After Balancing)')
ax2.set_xlabel('Category')
ax2.set_ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
st.pyplot(fig2)

# app_clean_text.py
import streamlit as st
import pandas as pd
import re
import unicodedata

st.title("Bangla Text Cleaning & Tokenization")

# ---------------------------
# Assume df_balanced is already created from previous processing step
# ---------------------------
st.subheader("Balanced Dataset Info Before Cleaning")
st.text(df_balanced.info())
st.dataframe(df_balanced.head(5))

# ---------------------------
# Bangla Stopword List
# ---------------------------
raw_stopwords = [
    "‡¶Ö‡¶¨‡¶∂‡ßç‡¶Ø", "‡¶Ö‡¶®‡ßç‡¶§‡¶§", "‡¶Ö‡¶•‡¶¨‡¶æ", "‡¶Ö‡¶•‡¶ö", "‡¶Ö‡¶∞‡ßç‡¶•‡¶æ‡¶§", "‡¶Ö‡¶®‡ßç‡¶Ø", "‡¶Ü‡¶ú", "‡¶Ü‡¶õ‡ßá", "‡¶Ü‡¶™‡¶®‡¶æ‡¶∞", "‡¶Ü‡¶™‡¶®‡¶ø", "‡¶Ü‡¶¨‡¶æ‡¶∞", "‡¶Ü‡¶Æ‡¶∞‡¶æ", "‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá", "‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞", "‡¶Ü‡¶Æ‡¶æ‡¶∞", "‡¶Ü‡¶Æ‡¶ø", "‡¶Ü‡¶∞‡¶ì", "‡¶Ü‡¶∞",
    "‡¶Ü‡¶ó‡ßá", "‡¶Ü‡¶ó‡ßá‡¶á","‡¶Ü‡¶ó‡¶æ‡¶Æ‡ßÄ", "‡¶Ö‡¶¨‡¶ß‡¶ø", "‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ", "‡¶Ü‡¶¶‡ßç‡¶Ø‡¶≠‡¶æ‡¶ó‡ßá", "‡¶è‡¶á", "‡¶è‡¶ï‡¶á", "‡¶è‡¶ï‡¶ï‡ßá", "‡¶è‡¶ï‡¶ü‡¶ø", "‡¶è‡¶ñ‡¶®", "‡¶è‡¶ñ‡¶®‡¶ì", "‡¶è‡¶ñ‡¶æ‡¶®‡ßá", "‡¶è‡¶ñ‡¶æ‡¶®‡ßá‡¶á", "‡¶è‡¶ü‡¶ø", "‡¶è‡¶ü‡¶æ", "‡¶è‡¶ü‡¶æ‡¶á", "‡¶è‡¶§‡¶ü‡¶æ‡¶á", "‡¶è‡¶¨‡¶Ç", "‡¶è‡¶ï‡¶¨‡¶æ‡¶∞",
    "‡¶è‡¶¨‡¶æ‡¶∞", "‡¶è‡¶¶‡ßá‡¶∞", "‡¶è‡¶Å‡¶¶‡ßá‡¶∞", "‡¶è‡¶Æ‡¶®", "‡¶è‡¶Æ‡¶®‡¶ï‡ßÄ", "‡¶è‡¶≤", "‡¶è‡¶∞", "‡¶è‡¶∞‡¶æ", "‡¶è‡¶Å‡¶∞‡¶æ", "‡¶è‡¶∏", "‡¶è‡¶§", "‡¶è‡¶§‡ßá", "‡¶è‡¶∏‡ßá", "‡¶è‡¶ï‡ßá", "‡¶è", "‡¶ê", "‡¶á", "‡¶á‡¶π‡¶æ", "‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø", "‡¶â‡¶®‡¶ø", "‡¶â‡¶™‡¶∞", "‡¶â‡¶™‡¶∞‡ßá",
    "‡¶â‡¶ö‡¶ø‡¶§", "‡¶ì", "‡¶ì‡¶á", "‡¶ì‡¶∞", "‡¶ì‡¶∞‡¶æ", "‡¶ì‡¶Å‡¶∞", "‡¶ì‡¶Å‡¶∞‡¶æ", "‡¶ì‡¶ï‡ßá", "‡¶ì‡¶¶‡ßá‡¶∞", "‡¶ì‡¶Å‡¶¶‡ßá‡¶∞", "‡¶ì‡¶ñ‡¶æ‡¶®‡ßá", "‡¶ï‡¶§", "‡¶ï‡¶¨‡ßá", "‡¶ï‡¶∞‡¶§‡ßá", "‡¶ï‡¶Ø‡¶º‡ßá‡¶ï", "‡¶ï‡¶Ø‡¶º‡ßá‡¶ï‡¶ü‡¶ø", "‡¶ï‡¶∞‡¶¨‡ßá", "‡¶ï‡¶∞‡¶≤‡ßá‡¶®", "‡¶ï‡¶∞‡¶æ‡¶∞", "‡¶ï‡¶æ‡¶∞‡¶ì",
    "‡¶ï‡¶∞‡¶æ", "‡¶ï‡¶∞‡¶ø", "‡¶ï‡¶∞‡¶ø‡¶Ø‡¶º‡ßá", "‡¶ï‡¶∞‡¶æ‡¶á", "‡¶ï‡¶∞‡¶≤‡ßá", "‡¶ï‡¶∞‡¶ø‡¶§‡ßá", "‡¶ï‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ", "‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡¶≤‡ßá‡¶®", "‡¶ï‡¶∞‡¶õ‡ßá", "‡¶ï‡¶∞‡¶õ‡ßá‡¶®", "‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶®", "‡¶ï‡¶∞‡ßá‡¶õ‡ßá", "‡¶ï‡¶∞‡ßá‡¶®", "‡¶ï‡¶∞‡¶¨‡ßá‡¶®", "‡¶ï‡¶∞‡¶æ‡¶Ø‡¶º", "‡¶ï‡¶∞‡ßá", "‡¶ï‡¶∞‡ßá‡¶á", "‡¶ï‡¶æ‡¶õ", "‡¶ï‡¶æ‡¶õ‡ßá",
    "‡¶ï‡¶æ‡¶∞‡¶£", "‡¶ï‡¶ø‡¶õ‡ßÅ", "‡¶ï‡¶ø‡¶õ‡ßÅ‡¶á", "‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ", "‡¶ï‡¶ø‡¶Ç‡¶¨‡¶æ", "‡¶ï‡¶ø", "‡¶ï‡ßÄ", "‡¶ï‡ßá‡¶â", "‡¶ï‡ßá‡¶â‡¶á", "‡¶ï‡¶æ‡¶â‡¶ï‡ßá", "‡¶ï‡ßá‡¶®", "‡¶ï‡ßá", "‡¶ï‡ßã‡¶®‡¶ì", "‡¶ï‡ßã‡¶®‡ßã", "‡¶ï‡ßã‡¶®", "‡¶ï‡¶ñ‡¶®‡¶ì", "‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá", "‡¶ñ‡ßÅ‡¶¨", "‡¶ó‡ßÅ‡¶≤‡¶ø", "‡¶ó‡¶ø‡¶Ø‡¶º‡ßá",
    "‡¶ó‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá", "‡¶ó‡ßá‡¶õ‡ßá", "‡¶ó‡ßá‡¶≤", "‡¶ó‡ßá‡¶≤‡ßá", "‡¶ó‡ßã‡¶ü‡¶æ", "‡¶ö‡¶≤‡ßá", "‡¶ö‡ßá‡¶Ø‡¶º‡ßá", "‡¶õ‡¶æ‡¶°‡¶º‡¶æ", "‡¶õ‡¶æ‡¶°‡¶º‡¶æ‡¶ì", "‡¶õ‡¶ø‡¶≤‡ßá‡¶®", "‡¶õ‡¶ø‡¶≤", "‡¶ú‡¶®‡ßç‡¶Ø", "‡¶ú‡¶æ‡¶®‡¶æ", "‡¶†‡¶ø‡¶ï", "‡¶§‡¶ø‡¶®‡¶ø", "‡¶§‡¶ø‡¶®‡¶ê", "‡¶§‡¶ø‡¶®‡¶ø‡¶ì", "‡¶§‡¶ñ‡¶®", "‡¶§‡¶¨‡ßá", "‡¶§‡¶¨‡ßÅ",
    "‡¶§‡¶æ‡¶Å‡¶¶‡ßá‡¶∞", "‡¶§‡¶æ‡¶Å‡¶π‡¶æ‡¶∞‡¶æ", "‡¶§‡¶æ‡¶Å‡¶∞‡¶æ", "‡¶§‡¶æ‡¶Å‡¶∞", "‡¶§‡¶æ‡¶Å‡¶ï‡ßá", "‡¶§‡¶æ‡¶á", "‡¶§‡ßá‡¶Æ‡¶®", "‡¶§‡¶æ‡¶ï‡ßá", "‡¶§‡¶æ‡¶π‡¶æ", "‡¶§‡¶æ‡¶π‡¶æ‡¶§‡ßá", "‡¶§‡¶æ‡¶π‡¶æ‡¶∞", "‡¶§‡¶æ‡¶¶‡ßá‡¶∞", "‡¶§‡¶æ‡¶∞‡¶™‡¶∞", "‡¶§‡¶æ‡¶∞‡¶æ", "‡¶§‡¶æ‡¶∞‡ßà", "‡¶§‡¶æ‡¶∞", "‡¶§‡¶æ‡¶π‡¶≤‡ßá", "‡¶§‡¶æ", "‡¶§‡¶æ‡¶ì", "‡¶§‡¶æ‡¶§‡ßá",
    "‡¶§‡ßã", "‡¶§‡¶§", "‡¶§‡ßÅ‡¶Æ‡¶ø", "‡¶§‡ßã‡¶Æ‡¶æ‡¶∞", "‡¶§‡¶•‡¶æ", "‡¶•‡¶æ‡¶ï‡ßá", "‡¶•‡¶æ‡¶ï‡¶æ", "‡¶•‡¶æ‡¶ï‡¶æ‡¶Ø‡¶º", "‡¶•‡ßá‡¶ï‡ßá", "‡¶•‡ßá‡¶ï‡ßá‡¶ì", "‡¶•‡¶æ‡¶ï‡¶¨‡ßá", "‡¶•‡¶æ‡¶ï‡ßá‡¶®", "‡¶•‡¶æ‡¶ï‡¶¨‡ßá‡¶®", "‡¶•‡ßá‡¶ï‡ßá‡¶á", "‡¶¶‡¶ø‡¶ï‡ßá", "‡¶¶‡¶ø‡¶§‡ßá", "‡¶¶‡¶ø‡¶Ø‡¶º‡ßá", "‡¶¶‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá", "‡¶¶‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡¶®",
    "‡¶¶‡ßÅ", "‡¶¶‡ßÅ‡¶ü‡¶ø", "‡¶¶‡ßÅ‡¶ü‡ßã", "‡¶¶‡ßá‡¶Ø‡¶º", "‡¶¶‡ßá‡¶Ø‡¶º‡¶æ", "‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ", "‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞", "‡¶¶‡ßá‡¶ñ‡¶æ", "‡¶¶‡ßá‡¶ñ‡ßá", "‡¶¶‡ßá‡¶ñ‡¶§‡ßá", "‡¶¶‡ßç‡¶¨‡¶æ‡¶∞‡¶æ", "‡¶ß‡¶∞‡ßá", "‡¶ß‡¶∞‡¶æ", "‡¶®‡¶Ø‡¶º", "‡¶®‡¶æ‡¶®‡¶æ", "‡¶®‡¶æ", "‡¶®‡¶æ‡¶ï‡¶ø", "‡¶®‡¶æ‡¶ó‡¶æ‡¶¶", "‡¶®‡¶ø‡¶§‡ßá", "‡¶®‡¶ø‡¶ú‡ßá","‡¶ï‡¶æ‡¶ú‡ßá",
    "‡¶®‡¶ø‡¶ú‡ßá‡¶á", "‡¶®‡¶ø‡¶ú‡ßá‡¶∞", "‡¶®‡¶ø‡¶ú‡ßá‡¶¶‡ßá‡¶∞", "‡¶®‡¶ø‡¶Ø‡¶º‡ßá", "‡¶®‡ßá‡¶ì‡¶Ø‡¶º‡¶æ", "‡¶®‡ßá‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞", "‡¶®‡ßá‡¶á", "‡¶®‡¶æ‡¶á", "‡¶™‡¶ï‡ßç‡¶∑‡ßá", "‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§", "‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ", "‡¶™‡¶æ‡¶∞‡ßá‡¶®", "‡¶™‡¶æ‡¶∞‡¶ø", "‡¶™‡¶æ‡¶∞‡ßá", "‡¶™‡¶∞‡ßá", "‡¶™‡¶∞‡ßá‡¶á", "‡¶™‡¶∞‡ßá‡¶ì", "‡¶™‡¶∞", "‡¶™‡ßá‡¶Ø‡¶º‡ßá", "‡¶™‡ßç‡¶∞‡¶§‡¶ø",
    "‡¶™‡ßç‡¶∞‡¶≠‡ßÉ‡¶§‡¶ø", "‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º", "‡¶´‡ßá‡¶∞", "‡¶´‡¶≤‡ßá", "‡¶´‡¶ø‡¶∞‡ßá", "‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞", "‡¶¨‡¶≤‡¶§‡ßá", "‡¶¨‡¶≤‡¶≤‡ßá‡¶®", "‡¶¨‡¶≤‡ßá‡¶õ‡ßá‡¶®", "‡¶¨‡¶≤‡¶≤", "‡¶¨‡¶≤‡¶æ", "‡¶¨‡¶≤‡ßá‡¶®", "‡¶¨‡¶≤‡ßá", "‡¶¨‡¶π‡ßÅ", "‡¶¨‡¶∏‡ßá", "‡¶¨‡¶æ‡¶∞", "‡¶¨‡¶æ", "‡¶¨‡¶ø‡¶®‡¶æ", "‡¶¨‡¶∞‡¶Ç", "‡¶¨‡¶¶‡¶≤‡ßá",
    "‡¶¨‡¶æ‡¶¶‡ßá", "‡¶¨‡¶ø‡¶∂‡ßá‡¶∑", "‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶®", "‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶ü‡¶ø", "‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞", "‡¶¨‡ßç‡¶Ø‡¶æ‡¶™‡¶æ‡¶∞‡ßá", "‡¶≠‡¶æ‡¶¨‡ßá", "‡¶≠‡¶æ‡¶¨‡ßá‡¶á", "‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá", "‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá‡¶á", "‡¶§‡ßã‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞", "‡¶§‡ßã‡¶Æ‡¶∞‡¶æ", "‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑", "‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑‡ßá‡¶∞", "‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá‡¶ì", "‡¶Æ‡¶ß‡ßç‡¶Ø‡¶≠‡¶æ‡¶ó‡ßá", "‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá", "‡¶Æ‡¶æ‡¶ù‡ßá",
    "‡¶Æ‡¶§‡ßã‡¶á", "‡¶Æ‡ßã‡¶ü‡ßá‡¶á", "‡¶Ø‡¶ñ‡¶®", "‡¶Ø‡¶¶‡¶ø", "‡¶Ø‡¶¶‡¶ø‡¶ì", "‡¶Ø‡¶æ‡¶¨‡ßá", "‡¶Ø‡¶æ‡¶Ø‡¶º", "‡¶Ø‡¶æ‡¶ï‡ßá", "‡¶Ø‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ", "‡¶Ø‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞", "‡¶Ø‡¶§", "‡¶Ø‡¶§‡¶ü‡¶æ", "‡¶Ø‡¶æ", "‡¶Ø‡¶æ‡¶∞", "‡¶Ø‡¶æ‡¶∞‡¶æ", "‡¶Ø‡¶æ‡¶Å‡¶∞", "‡¶Ø‡¶æ‡¶Å‡¶∞‡¶æ", "‡¶Ø‡¶æ‡¶¶‡ßá‡¶∞", "‡¶Ø‡¶æ‡¶®", "‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá",
    "‡¶Ø‡ßá‡¶§‡ßá", "‡¶Ø‡¶æ‡¶§‡ßá", "‡¶Ø‡ßá‡¶®", "‡¶Ø‡ßá‡¶Æ‡¶®", "‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá", "‡¶Ø‡¶ø‡¶®‡¶ø", "‡¶Ø‡ßá", "‡¶∞‡ßá‡¶ñ‡ßá", "‡¶∞‡¶æ‡¶ñ‡¶æ", "‡¶∞‡¶Ø‡¶º‡ßá‡¶õ‡ßá", "‡¶∞‡¶ï‡¶Æ", "‡¶∂‡ßÅ‡¶ß‡ßÅ", "‡¶∏‡¶ô‡ßç‡¶ó‡ßá", "‡¶∏‡¶ô‡ßç‡¶ó‡ßá‡¶ì", "‡¶∏‡¶Æ‡¶∏‡ßç‡¶§", "‡¶∏‡¶¨", "‡¶∏‡¶¨‡¶æ‡¶∞", "‡¶∏‡¶π", "‡¶∏‡ßÅ‡¶§‡¶∞‡¶æ‡¶Ç", "‡¶∏‡¶π‡¶ø‡¶§",
    "‡¶∏‡ßá‡¶á", "‡¶∏‡ßá‡¶ü‡¶æ", "‡¶∏‡ßá‡¶ü‡¶ø", "‡¶∏‡ßá‡¶ü‡¶æ‡¶á", "‡¶∏‡ßá‡¶ü‡¶æ‡¶ì", "‡¶∏‡¶Æ‡ßç‡¶™‡ßç‡¶∞‡¶§‡¶ø", "‡¶∏‡ßá‡¶ñ‡¶æ‡¶®", "‡¶∏‡ßá‡¶ñ‡¶æ‡¶®‡ßá", "‡¶∏‡ßá", "‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü", "‡¶∏‡ßç‡¶¨‡¶Ø‡¶º‡¶Ç", "‡¶π‡¶á‡¶§‡ßá", "‡¶π‡¶á‡¶¨‡ßá", "‡¶π‡ßà‡¶≤‡ßá", "‡¶π‡¶á‡¶Ø‡¶º‡¶æ", "‡¶π‡¶ö‡ßç‡¶õ‡ßá", "‡¶π‡¶§", "‡¶ï‡ßã‡¶®‡¶ü‡¶ø", "‡¶π‡¶§‡ßá", "‡¶π‡¶§‡ßá‡¶á",
    "‡¶π‡¶¨‡ßá", "‡¶π‡¶¨‡ßá‡¶®", "‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤", "‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá", "‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡¶®", "‡¶π‡¶Ø‡¶º‡ßá", "‡¶π‡¶Ø‡¶º‡¶®‡¶ø", "‡¶π‡¶Ø‡¶º", "‡¶π‡¶Ø‡¶º‡ßá‡¶á", "‡¶π‡¶Ø‡¶º‡¶§‡ßã", "‡¶π‡¶≤", "‡¶π‡¶≤‡ßá", "‡¶π‡¶≤‡ßá‡¶á", "‡¶π‡¶≤‡ßá‡¶ì", "‡¶π‡¶≤‡ßã", "‡¶π‡¶ø‡¶∏‡¶æ‡¶¨‡ßá", "‡¶π‡¶ì‡¶Ø‡¶º‡¶æ", "‡¶π‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞", "‡¶π‡¶ì‡¶Ø‡¶º‡¶æ‡¶Ø‡¶º", "‡¶π‡¶®",
    "‡¶π‡ßã‡¶ï", "‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º", "‡¶∂‡ßã‡¶®‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º", "‡¶ó‡¶§", "‡¶®‡¶ø‡¶Ø‡¶º‡ßá", "‡¶Ø‡¶æ‡¶Ø‡¶º", "‡¶π‡¶Ø‡¶º‡ßá", "‡¶ï‡¶•‡¶æ", "‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ", "‡¶ï‡¶æ‡¶ú", "‡¶§‡ßà‡¶∞‡¶ø", "‡¶ú‡¶æ‡¶®‡¶æ‡¶®", "‡¶¶‡¶ø‡¶Ø‡¶º‡ßá", "‡¶ú‡¶æ‡¶®‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá", "‡ß¶", "‡ßß", "‡ßß‡ß¶", "‡ßß‡ßß", "‡ßß‡ß®", "‡ßß‡ß©",
    "‡ßß‡ß™", "‡ß®", "‡ß©", "‡ß™", "‡ß´", "‡ß¨", "‡ß≠", "‡ßÆ", "‡ßØ", "‡ßß‡ßß", "‡ßß‡ß®", "‡ßß‡ß©", "‡ßß‡ß™", "‡ßß‡ß´", "‡ßß‡ß¨", "‡ßß‡ß≠", "‡ßß‡ßÆ", "‡ßß‡ßØ", "‡ß®‡ß¶","‡ß®‡ßß", "‡ß®‡ß®", "‡ß®‡ß©", "‡ß®‡ß™", "‡ß®‡ß´", "‡ß®‡ß¨", "‡ß®‡ß≠", "‡ß®‡ßÆ", "‡ß®‡ßØ", "‡ß©‡ß¶",
    "‡ß©‡ßß", "‡ß©‡ß®", "‡ß©‡ß©", "‡ß©‡ß™", "‡ß©‡ß´", "‡ß©‡ß¨", "‡ß©‡ß≠", "‡ß©‡ßÆ", "‡ß©‡ßØ", "‡ß™‡ß¶","‡ß™‡ßß", "‡ß™‡ß®", "‡ß™‡ß©", "‡ß™‡ß™", "‡ß™‡ß´", "‡ß™‡ß¨", "‡ß™‡ß≠", "‡ß™‡ßÆ", "‡ß™‡ßØ", "‡ß´‡ß¶","‡ß´‡ßß", "‡ß´‡ß®", "‡ß´‡ß©", "‡ß´‡ß™", "‡ß´‡ß´", "‡ß´‡ß¨", "‡ß´‡ß≠", "‡ß´‡ßÆ", "‡ß´‡ßØ", "‡ß¨‡ß¶",
    "‡ß¨‡ßß", "‡ß¨‡ß®", "‡ß¨‡ß©", "‡ß¨‡ß™", "‡ß¨‡ß´", "‡ß¨‡ß¨", "‡ß¨‡ß≠", "‡ß¨‡ßÆ", "‡ß¨‡ßØ", "‡ß≠‡ß¶","‡ß≠‡ßß", "‡ß≠‡ß®", "‡ß≠‡ß©", "‡ß≠‡ß™", "‡ß≠‡ß´", "‡ß≠‡ß¨", "‡ß≠‡ß≠", "‡ß≠‡ßÆ", "‡ß≠‡ßØ", "‡ßÆ‡ß¶",
    "‡ßÆ‡ßß", "‡ßÆ‡ß®", "‡ßÆ‡ß©", "‡ßÆ‡ß™", "‡ßÆ‡ß´", "‡ßÆ‡ß¨", "‡ßÆ‡ß≠", "‡ßÆ‡ßÆ", "‡ßÆ‡ßØ", "‡ßØ‡ß¶","‡ßØ‡ßß", "‡ßØ‡ß®", "‡ßØ‡ß©", "‡ßØ‡ß™", "‡ßØ‡ß´", "‡ßØ‡ß¨", "‡ßØ‡ß≠", "‡ßØ‡ßÆ", "‡ßØ‡ßØ", "‡ßß‡ß¶‡ß¶", "‡¶Ü‡¶≤‡ßã", "‡¶è‡¶ï", "‡¶è‡¶ï‡¶ú‡¶®", "‡¶è‡¶ï‡¶ü‡ßÅ", "‡¶ì‡¶™‡¶∞", "‡¶ñ‡¶æ‡¶®", "‡¶ï‡¶æ‡¶ú‡ßá‡¶∞ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá", "‡¶ï‡¶æ‡¶∞‡¶£‡ßá", "‡¶ï‡¶∞‡ßã", "‡¶ï‡¶∞‡ßÅ‡¶®", "‡¶ï‡¶Æ","‡¶¶‡¶ø‡¶≤‡ßá‡¶®","‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø", "‡¶∏‡ßÅ‡¶Ø‡ßã‡¶ó",
    "‡¶ï‡¶Æ‡ßá‡¶õ‡ßá", "‡¶ö‡ßå‡¶ß‡ßÅ‡¶∞‡ßÄ", "‡¶õ‡¶Ø‡¶º", "‡¶õ‡ßã‡¶ü", "‡¶ú‡¶æ‡¶®‡¶æ‡¶Ø‡¶º", "‡¶ú‡¶æ‡¶®‡¶æ‡¶®", "‡¶ú‡¶®", "‡¶ö‡¶æ‡¶∞", "‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§", "‡¶°:", "‡¶¶‡¶ø‡¶®", "‡¶¶‡¶∂", "‡¶¶‡ßÅ‡¶á", "‡¶®‡¶§‡ßÅ‡¶®", "‡¶∂‡ßá‡¶∑", "‡¶®‡¶ø‡¶≤‡ßá", "‡¶®‡¶ø‡¶®", "‡¶®‡¶Ø‡¶º", "‡¶™‡¶æ‡¶¨‡ßá‡¶®","‡¶Æ‡¶æ‡¶§‡ßç‡¶∞", "‡¶Æ‡¶§‡ßã",
    "‡¶™‡ßá‡¶§‡ßá", "‡¶™‡¶æ‡¶∞‡¶¨‡ßá‡¶®", "‡¶¶‡ßÇ‡¶∞‡ßá", "‡¶Ø‡ßá‡¶ï‡ßã‡¶®‡ßã", "‡¶•‡¶æ‡¶ï‡¶≤‡ßá", "‡¶∏‡¶Æ‡ßç‡¶≠‡¶æ‡¶¨‡¶®‡¶æ", "‡¶è‡¶ï‡¶ü‡¶æ", "‡¶∂‡ßÅ‡¶≠", "‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£", "‡¶•‡¶æ‡¶ï‡¶§‡ßá", "‡¶∞‡¶æ‡¶ñ‡ßÅ‡¶®", "‡¶ñ‡ßá‡¶§‡ßá", "‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø", "‡¶ò‡¶ü‡¶®‡¶æ", "‡¶™‡ßç‡¶∞‡¶•‡¶Æ", "‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶®", "‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∂", "‡¶¨‡¶õ‡¶∞", "‡¶¨‡¶°‡¶º",
    "‡¶¨‡ßá‡¶∂","‡¶¨‡ßá‡¶∂‡¶ø", "‡¶Æ‡¶®‡ßá", "‡¶Æ‡ßã", "‡¶Æ‡ßã‡¶É", "‡¶™‡ßç‡¶∞‡¶•‡¶Æ", "‡¶¶‡ßç‡¶¨‡¶ø‡¶§‡ßÄ‡¶Ø‡¶º", "‡¶§‡ßÉ‡¶§‡ßÄ‡¶Ø‡¶º", "‡¶ö‡¶§‡ßÅ‡¶∞‡ßç‡¶•", "‡¶™‡¶û‡ßç‡¶ö‡¶Æ", "‡¶∑‡¶∑‡ßç‡¶†", "‡¶∏‡¶™‡ßç‡¶§‡¶Æ", "‡¶Ö‡¶∑‡ßç‡¶ü‡¶Æ", "‡¶®‡¶¨‡¶Æ", "‡¶¶‡¶∂‡¶Æ", "‡¶è‡¶ï‡¶æ‡¶¶‡¶∂", "‡¶¶‡ßç‡¶¨‡¶æ‡¶¶‡¶∂", "‡¶§‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶¶‡¶∂", "‡¶ö‡¶§‡ßÅ‡¶∞‡ßç‡¶¶‡¶∂", "‡¶™‡¶û‡ßç‡¶ö‡¶¶‡¶∂", "‡¶∑‡ßã‡¶°‡¶º‡¶∂",
    "‡¶∏‡¶™‡ßç‡¶§‡¶¶‡¶∂", "‡¶Ö‡¶∑‡ßç‡¶ü‡¶æ‡¶¶‡¶∂", "‡¶ä‡¶®‡¶¨‡¶ø‡¶Ç‡¶∂", "‡¶¨‡¶ø‡¶∂‡¶§‡¶Æ", "‡ßß‡¶Æ", "‡ß®‡¶Ø‡¶º", "‡ß©‡¶Ø‡¶º", "‡ß™‡¶∞‡ßç‡¶•", "‡ß´‡¶Æ", "‡ß¨‡¶∑‡ßç‡¶†", "‡ß≠‡¶Æ", "‡ßÆ‡¶Æ", "‡ßØ‡¶Æ", "‡ßß‡ß¶‡¶Æ", "‡ßß‡ßß‡¶§‡¶Æ", "‡ßß‡ß®‡¶§‡¶Æ", "‡ßß‡ß©‡¶§‡¶Æ", "‡ßß‡ß™‡¶§‡¶Æ", "‡ßß‡ß´‡¶§‡¶Æ", "‡ßß‡ß¨‡¶§‡¶Æ",
    "‡ßß‡ß≠‡¶§‡¶Æ", "‡ßß‡ßÆ‡¶§‡¶Æ", "‡ßß‡ßØ‡¶§‡¶Æ", "‡ß®‡ß¶‡¶§‡¶Æ", "‡ß®‡ßß‡¶§‡¶Æ", "‡ß®‡ß®‡¶§‡¶Æ", "‡ß®‡ß©‡¶§‡¶Æ", "‡ß®‡ß™‡¶§‡¶Æ", "‡ß®‡ß´‡¶§‡¶Æ", "‡ß®‡ß¨‡¶§‡¶Æ", "‡ß®‡ß≠‡¶§‡¶Æ", "‡ß®‡ßÆ‡¶§‡¶Æ", "‡ß®‡ßØ‡¶§‡¶Æ", "‡ß©‡ß¶‡¶§‡¶Æ", "‡ß©‡ßß‡¶§‡¶Æ", "‡ß©‡ß®‡¶§‡¶Æ", "‡ß©‡ß©‡¶§‡¶Æ", "‡ß©‡ß™‡¶§‡¶Æ",
    "‡ß©‡ß´‡¶§‡¶Æ", "‡ß©‡ß¨‡¶§‡¶Æ","‡ß©‡ß≠‡¶§‡¶Æ", "‡ß©‡ßÆ‡¶§‡¶Æ", "‡ß©‡ßØ‡¶§‡¶Æ", "‡ß™‡ß¶‡¶§‡¶Æ", "‡ß™‡ßß‡¶§‡¶Æ", "‡ß™‡ß®‡¶§‡¶Æ", "‡ß™‡ß©‡¶§‡¶Æ", "‡ß™‡ß™‡¶§‡¶Æ", "‡ß™‡ß´‡¶§‡¶Æ", "‡ß™‡ß¨‡¶§‡¶Æ", "‡ß™‡ß≠‡¶§‡¶Æ", "‡ß™‡ßÆ‡¶§‡¶Æ", "‡ß™‡ßØ‡¶§‡¶Æ", "‡ß´‡ß¶‡¶§‡¶Æ", "‡¶∂‡¶§‡¶æ‡¶Ç‡¶∂", "‡¶ö‡¶æ‡¶≤‡ßÅ", "‡¶ï‡ßã‡¶ü‡¶ø", "‡¶¶‡ßá‡¶∂‡ßá‡¶∞",
    "‡¶¶‡ßá‡¶∂", "‡¶∂‡¶§","‡¶π‡¶æ‡¶ú‡¶æ‡¶∞", "‡¶≤‡¶æ‡¶ñ", "‡¶ï‡ßã‡¶ü‡¶ø", "‡¶Æ‡¶ø‡¶≤‡¶ø‡¶Ø‡¶º‡¶®", "‡¶¨‡¶ø‡¶≤‡¶ø‡¶Ø‡¶º‡¶®", "‡¶¨‡¶õ‡¶∞", "‡¶¨‡¶õ‡¶∞‡ßá‡¶∞", "‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ", "‡¶™‡¶æ‡¶∂‡¶æ‡¶™‡¶æ‡¶∂‡¶ø", "‡¶∏‡ßá‡¶¨‡¶æ", "‡¶∂‡¶§", "‡¶ö‡ßá‡¶Ø‡¶º‡¶æ‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®", "‡¶™‡¶∞‡¶ø‡¶ö‡¶æ‡¶≤‡¶ï", "‡¶¨‡¶ø‡¶∞‡ßÅ‡¶¶‡ßç‡¶ß‡ßá", "‡¶ñ‡¶¨‡¶∞", "‡¶Ö‡¶≠‡¶ø‡¶Ø‡ßã‡¶ó", "‡¶∏‡¶æ‡¶≤‡ßá‡¶∞",
    "‡¶π‡ßã‡¶∏‡ßá‡¶®", "‡¶ß‡¶∞‡¶®‡ßá‡¶∞","‡¶∞‡¶π‡¶Æ‡¶æ‡¶®", "‡¶∏‡¶æ‡¶≤‡ßá", "‡¶Ö‡¶®‡ßÅ‡¶∑‡ßç‡¶†‡¶æ‡¶®", "‡¶Ö‡¶®‡ßÅ‡¶∑‡ßç‡¶†‡¶æ‡¶®‡ßá", "‡¶Ö‡¶®‡ßá‡¶ï", "‡¶ï‡¶Æ‡ßá", "‡¶¶‡ßá‡¶®", "‡¶â‡¶™‡¶∏‡ßç‡¶•‡¶ø‡¶§", "‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞‡ßá‡¶∞", "‡¶¨‡ßá‡¶°‡¶º‡ßá‡¶õ‡ßá", "‡¶¶‡ßá‡¶∂‡ßá", "‡¶∂‡ßÅ‡¶∞‡ßÅ", "‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá", "‡¶Æ‡ßã‡¶ü", "‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£", "‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡ßá", "‡¶∏‡¶≠‡¶æ‡¶Ø‡¶º", "‡¶Ö‡¶Ç‡¶∂",
    "‡¶è‡¶Æ", "‡¶Ü‡¶∞‡ßã", "‡¶∏‡ßÅ‡¶Ø‡ßã‡¶ó", "‡¶è‡¶ï", "‡¶¶‡ßÅ‡¶á", "‡¶§‡¶ø‡¶®", "‡¶ö‡¶æ‡¶∞", "‡¶™‡¶æ‡¶Å‡¶ö", "‡¶õ‡¶Ø‡¶º", "‡¶∏‡¶æ‡¶§", "‡¶Ü‡¶ü", "‡¶®‡¶Ø‡¶º", "‡¶¶‡¶∂", "‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶®‡ßá", "‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º", "‡¶Ö‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°", "‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá", "‡¶™‡¶°‡¶º‡ßá", "‡¶∏‡¶Æ‡¶Ø‡¶º", "‡¶ï", "‡¶ñ", "‡¶ó",
    "‡¶ò", "‡¶ô", "‡¶ö","‡¶õ", "‡¶ú", "‡¶ù", "‡¶û", "‡¶ü", "‡¶†", "‡¶°", "‡¶¢", "‡¶¶", "‡¶™", "‡¶´", "‡¶¨", "‡¶≠", "‡¶Æ", "‡¶Ø", "‡¶∞", "‡¶≤", "‡¶∂", "‡¶∑", "‡¶∏", "‡¶π", "‡¶ï‡ßç‡¶∑", "‡¶°‡¶º", "‡¶¢‡¶º", "‡¶Ø‡¶º", "‡ßé", "‡¶Ö", "‡¶Ü", "‡¶á",
    "‡¶à", "‡¶â", "‡¶ä", "‡¶è", "‡¶ê", "‡¶ì", "‡¶î","‡¶∏‡ßÉ‡¶∑‡ßç‡¶ü‡¶ø", "‡¶π‡¶æ‡¶§‡ßá", "‡¶è‡¶Æ‡¶®‡¶ï‡¶ø", "‡¶∏‡¶æ‡¶Æ‡¶®‡ßá", "‡¶è‡¶∏‡¶¨", "‡¶§‡ßÅ‡¶≤‡ßá", "‡¶ó‡¶°‡¶º‡ßá", "‡¶Ø‡ßá‡¶∏‡¶¨", "‡¶∏‡ßá‡¶∏‡¶¨", "‡¶¨‡¶®‡ßç‡¶ß", "‡¶ñ‡ßã‡¶≤‡¶æ", "‡¶∂‡ßÅ‡¶∞‡ßÅ", "‡¶∂‡ßá‡¶∑", "‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ", "‡¶∏‡¶æ‡¶´‡¶≤‡ßç‡¶Ø", "‡¶∏‡¶´‡¶≤‡¶§‡¶æ",
    "‡¶Ü‡¶∂‡¶æ","‡¶∏‡¶æ‡¶Æ‡¶®‡ßá", "‡¶™‡¶ø‡¶õ‡¶®‡ßá", "‡¶™‡¶æ‡¶∞‡¶¨‡ßá", "‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞‡ßá", "‡¶π‡¶æ‡¶§‡ßá", "‡¶π‡¶§‡ßá", "‡¶è‡¶ó‡¶ø‡¶Ø‡¶º‡ßá", "‡¶è‡¶∞‡¶™‡¶∞", "‡¶§‡¶æ‡¶∞‡¶™‡¶∞", "‡¶Ö‡¶§‡¶É‡¶™‡¶∞","‡¶®‡ßá‡¶®", "‡¶∂‡ßá‡¶∑‡ßá", "‡¶∂‡ßÅ‡¶∞‡¶§‡ßá", "‡¶§‡ßÅ‡¶≤‡ßá", "‡¶™‡¶æ‡¶∞‡ßá‡¶®‡¶®‡¶ø", "‡¶ï‡¶æ‡¶≤", "‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá", "‡¶ú‡¶æ‡¶®‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡¶®",
    "‡¶ú‡¶æ‡¶®‡¶ø‡¶Ø‡¶º‡ßá","‡¶ú‡¶æ‡¶®‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤", "‡¶ú‡¶æ‡¶®‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤‡ßá", "‡¶≤‡¶ø‡¶ñ‡ßá‡¶õ‡ßá‡¶®", "‡¶≤‡¶ø‡¶ñ‡¶§‡ßá", "‡¶ö‡¶æ‡¶á", "‡¶∂‡ßÅ‡¶∞‡ßÅ", "‡¶∂‡¶®‡¶ø‡¶¨‡¶æ‡¶∞", "‡¶∞‡¶¨‡¶ø‡¶¨‡¶æ‡¶∞", "‡¶∏‡ßã‡¶Æ‡¶¨‡¶æ‡¶∞","‡¶Æ‡¶ô‡ßç‡¶ó‡¶≤‡¶¨‡¶æ‡¶∞", "‡¶¨‡ßÅ‡¶ß‡¶¨‡¶æ‡¶∞", "‡¶¨‡ßÉ‡¶π‡¶É‡¶∏‡ßç‡¶™‡¶§‡¶ø‡¶¨‡¶æ‡¶∞", "‡¶¨‡ßÉ‡¶π‡¶∏‡ßç‡¶™‡¶§‡¶ø‡¶¨‡¶æ‡¶∞", "‡¶∂‡ßÅ‡¶ï‡ßç‡¶∞‡¶¨‡¶æ‡¶∞",  "‡¶∏‡¶ø‡¶®‡ßç‡¶ß‡¶æ‡¶®‡ßç‡¶§", "‡¶Ü‡¶õ‡ßá‡¶®",  "‡¶∞‡¶æ‡¶§‡ßá", "‡¶¶‡ßÅ‡¶™‡ßÅ‡¶∞‡ßá", "‡¶ú‡¶æ‡¶®‡¶§‡ßá",
    "‡¶¶‡¶æ‡¶¨‡¶ø", "‡¶∏‡¶æ‡¶•‡ßá", "‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶Ø‡¶º", "‡¶®‡¶Æ‡ßç‡¶¨‡¶∞", "‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ", "‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ‡ßá‡¶∞", "‡¶∂‡¶π‡¶∞", "‡¶∂‡¶π‡¶∞‡ßá‡¶∞","‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡ßç‡¶•‡¶æ", "‡¶¨‡¶æ‡¶°‡¶º‡¶ø", "‡¶¨‡¶æ‡¶°‡¶º‡¶ø‡¶§‡ßá", "‡¶ö‡¶æ‡¶≤‡¶ø‡¶Ø‡¶º‡ßá", "‡¶ú‡¶®‡¶ï‡ßá", "‡¶ò‡¶ü‡¶®‡¶æ‡¶∞", "‡¶∏‡¶¶‡¶∞", "‡¶®‡¶Æ‡ßç‡¶¨‡¶∞", "‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶", "‡¶™‡¶§‡ßç‡¶∞‡¶ø‡¶ï‡¶æ", "‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§",
    "‡¶∏‡¶π‡¶ï‡¶æ‡¶∞‡ßÄ", "‡¶õ‡ßá‡¶≤‡ßá", "‡¶Æ‡ßá‡¶Ø‡¶º‡ßá", "‡¶è‡¶∏‡¶Æ‡¶Ø‡¶º", "‡¶™‡¶æ‡¶†‡¶æ‡¶®‡ßã", "‡¶®‡¶ø‡¶ö‡ßá‡¶∞", "‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø", "‡¶∏‡¶¶‡¶∏‡ßç‡¶Ø", "‡¶¨‡¶æ‡¶ï‡¶ø", "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂", "‡¶ò‡ßã‡¶∑‡¶£‡¶æ","‡¶≠‡ßÇ‡¶Æ‡¶ø‡¶ï‡¶æ", "‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶®", "‡¶™‡¶∞‡¶ø‡¶Æ‡¶æ‡¶£", "‡¶Ö‡¶∞‡ßç‡¶•", "‡¶¶‡¶æ‡¶ì", "‡¶®‡¶æ‡¶Æ‡ßá", "‡¶¢‡¶æ‡¶ï‡¶æ", "‡¶ö‡¶ü‡ßç‡¶ü‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ", "‡¶ï‡ßÅ‡¶Æ‡¶ø‡¶≤‡ßç‡¶≤‡¶æ", "‡¶ú‡¶æ‡¶®‡ßÅ‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø",
    "‡¶´‡ßá‡¶¨‡ßç‡¶∞‡ßÅ‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø", "‡¶Æ‡¶æ‡¶∞‡ßç‡¶ö", "‡¶è‡¶™‡ßç‡¶∞‡¶ø‡¶≤", "‡¶Æ‡ßá", "‡¶ú‡ßÅ‡¶®", "‡¶ú‡ßÅ‡¶≤‡¶æ‡¶á", "‡¶Ü‡¶ó‡¶∑‡ßç‡¶ü", "‡¶∏‡ßá‡¶™‡ßç‡¶ü‡ßá‡¶Æ‡ßç‡¶¨‡¶∞", "‡¶Ö‡¶ï‡ßç‡¶ü‡ßã‡¶¨‡¶∞", "‡¶®‡¶≠‡ßá‡¶Æ‡ßç‡¶¨‡¶∞","‡¶°‡¶ø‡¶∏‡ßá‡¶Æ‡ßç‡¶¨‡¶∞", "‡¶¨‡ßà‡¶∂‡¶æ‡¶ñ", "‡¶ú‡ßà‡¶∑‡ßç‡¶†‡ßç‡¶Ø", "‡¶Ü‡¶∑‡¶æ‡¶¢‡¶º", "‡¶∂‡ßç‡¶∞‡¶æ‡¶¨‡¶£", "‡¶≠‡¶æ‡¶¶‡ßç‡¶∞", "‡¶Ü‡¶∂‡ßç‡¶¨‡¶ø‡¶®", "‡¶ï‡¶æ‡¶∞‡ßç‡¶§‡ßç‡¶§‡¶ø‡¶ï", "‡¶Ö‡¶ó‡ßç‡¶∞‡¶π‡¶æ‡¶Ø‡¶º‡¶®", "‡¶™‡ßå‡¶∑",
    "‡¶Æ‡¶æ‡¶ò", "‡¶´‡¶æ‡¶≤‡ßç‡¶ó‡ßÅ‡¶®", "‡¶ö‡ßà‡¶§‡ßç‡¶∞",  "‡¶®‡¶æ‡¶Æ", "‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø", "‡¶∞‡¶æ‡¶ñ‡¶§‡ßá", "‡¶¶‡ßá‡¶¨‡ßá", "‡¶¶‡¶æ‡¶Æ", "‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶π‡ßÄ", "‡¶∏‡¶π‡¶ú", "‡¶¨‡¶≤‡¶õ‡ßá","‡¶∏‡¶Æ‡¶Ø‡¶º‡ßá", "‡¶è‡¶∏‡ßá‡¶õ‡ßá", "‡¶â‡¶®‡ßç‡¶®‡¶§", "‡¶Ü‡¶™‡¶®‡¶æ‡¶ï‡ßá", "‡¶≤‡¶æ‡¶ó‡¶æ‡¶®", "‡¶≤‡¶æ‡¶ó‡¶ø‡¶Ø‡¶º‡ßá", "‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ‡¶ø‡¶§",
    "‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø", "‡¶π‡¶ï", "‡¶∏‡¶æ‡¶°‡¶º‡ßá", "‡¶¶‡¶æ‡¶Ø‡¶º‡¶ø‡¶§‡ßç‡¶¨", "‡¶ó‡ßç‡¶∞‡¶π‡¶£", "‡¶ò‡¶ü‡¶®‡¶æ‡¶Ø‡¶º"
]
bangla_stopwords = set(unicodedata.normalize("NFC", word.strip()) for word in raw_stopwords)

# ---------------------------
# Cleaning Function
# ---------------------------
def clean_text(text):
    if pd.isnull(text):
        return ""

    # Normalize
    text = unicodedata.normalize("NFC", text)

    # Remove non-Bangla characters
    text = re.sub(r'[^\u0980-\u09FF\s]', '', text)

    # Remove URLs
    text = re.sub(r'http\S+|www.\S+', '', text)

    # Tokenize
    tokens = text.split()

    # Remove stopwords
    filtered = [t for t in tokens if t not in bangla_stopwords]

    return ' '.join(filtered)

# ---------------------------
# Apply cleaning on balanced dataset
# ---------------------------
st.write("Cleaning text... this may take a few seconds for large datasets.")
df_balanced['cleaned_content'] = df_balanced['content'].apply(clean_text)

st.subheader("Dataset After Cleaning")
st.dataframe(df_balanced[['content', 'cleaned_content']].head(10))
st.write("Total rows:", df_balanced.shape[0])

# app_class_stopwords.py
import streamlit as st
import pandas as pd
import re

st.title("Bangla Class-Specific Stopword Removal & Tokenization")

# ---------------------------
# Assume df_balanced is already cleaned (from previous step)
# ---------------------------
st.subheader("Dataset Before Class-Specific Stopword Removal")
st.dataframe(df_balanced[['category', 'cleaned_content']].head(5))

# ---------------------------
# Words to remove per class
# ---------------------------
class_word_map = {
    'technology': ['‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶Æ‡¶®‡ßç‡¶§‡ßç‡¶∞‡ßÄ', '‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï', '‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡ßá','‡¶§‡¶•‡ßç‡¶Ø', '‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞', '‡¶ü‡¶æ‡¶ï‡¶æ', '‡¶®‡¶æ‡¶Æ‡ßá‡¶∞', '‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ', '‡¶™‡¶£‡ßç‡¶Ø'],
    'economy': ['‡¶õ‡¶æ‡¶§‡ßç‡¶∞‡¶≤‡ßÄ‡¶ó‡ßá‡¶∞','‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶∑‡ßç‡¶†‡¶æ‡¶®', '‡¶§‡¶•‡ßç‡¶Ø', '‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞'],
    'entertainment': ['‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶Æ‡¶®‡ßç‡¶§‡ßç‡¶∞‡ßÄ', '‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï', '‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡ßá','‡¶∏‡¶æ‡¶Æ‡¶æ‡¶ú‡¶ø‡¶ï','‡¶™‡ßã‡¶∏‡ßç‡¶ü'],
    'health': ['‡¶ò‡¶®‡ßç‡¶ü‡¶æ‡¶Ø‡¶º','‡¶ó‡ßá‡¶õ‡ßá‡¶®','‡¶¶‡¶æ‡¶Å‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá','‡¶ú‡¶æ‡¶®‡¶æ‡¶®‡ßã','‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶™‡ßç‡¶§‡¶ø', '‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ','‡¶¨‡¶æ‡¶á‡¶∞‡ßá'],
    'education': ['‡¶∏‡¶≠‡¶æ‡¶™‡¶§‡¶ø','‡¶á‡¶∏‡¶≤‡¶æ‡¶Æ','‡¶∂‡ßá‡¶ñ', '‡¶ï‡¶Æ‡¶ø‡¶ü‡¶ø', '‡¶õ‡¶æ‡¶§‡ßç‡¶∞‡¶≤‡ßÄ‡¶ó‡ßá‡¶∞','‡¶Ö‡¶®‡ßÅ‡¶∑‡ßç‡¶†‡¶ø‡¶§','‡¶§‡¶•‡ßç‡¶Ø', '‡¶è‡¶¶‡¶ø‡¶ï‡ßá', '‡¶∏‡ßÇ‡¶§‡ßç‡¶∞'],
    'crime': ['‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ','‡¶á‡¶∏‡¶≤‡¶æ‡¶Æ','‡¶Ü‡¶≤‡ßã‡¶ï‡ßá', '‡¶è‡¶≤‡¶æ‡¶ï‡¶æ‡¶∞'],
    'lifestyle': ['‡¶ü‡¶æ‡¶ï‡¶æ'],
    'environment': ['‡¶§‡¶•‡ßç‡¶Ø'],
}

# ---------------------------
# Function to remove class-specific words
# ---------------------------
def remove_class_words(row):
    text = row['cleaned_content']
    class_name = row['category']

    if class_name in class_word_map:
        for word in class_word_map[class_name]:
            text = text.replace(word, '')
        text = re.sub(r'\s+', ' ', text).strip()
    return text

# ---------------------------
# Apply to DataFrame
# ---------------------------
st.write("Removing class-specific words...")
df_balanced['cleaned_content'] = df_balanced.apply(remove_class_words, axis=1)

st.subheader("Dataset After Class-Specific Stopword Removal")
st.dataframe(df_balanced[['category', 'cleaned_content']].head(5))

# ---------------------------
# Tokenization
# ---------------------------
st.write("Tokenizing cleaned content into words...")
df_balanced['token_list'] = df_balanced['cleaned_content'].apply(lambda x: x.split())

st.subheader("Dataset with Token Lists")
st.dataframe(df_balanced[['category', 'token_list']].head(5))


# app_unique_words.py
import streamlit as st
from collections import defaultdict

st.title("Unique Words per Bangla News Category")

# ---------------------------
# Assume df_balanced has a 'token_list' column from previous step
# ---------------------------
st.subheader("Tokenized Dataset Sample")
st.dataframe(df_balanced[['category', 'token_list']].head(5))

# ---------------------------
# Step 1: Collect sets of unique words per category
# ---------------------------
category_word_sets = defaultdict(set)

for category in df_balanced['category'].unique():
    tokens = df_balanced[df_balanced['category'] == category]['token_list']
    all_tokens = [token for token_list in tokens for token in token_list]
    category_word_sets[category] = set(all_tokens)

# ---------------------------
# Step 2: Create a global word-to-category mapping
# ---------------------------
word_category_count = defaultdict(set)

for category, word_set in category_word_sets.items():
    for word in word_set:
        word_category_count[word].add(category)

# ---------------------------
# Step 3: Keep only words that appear in a single category
# ---------------------------
unique_words_by_category = defaultdict(list)

for word, categories in word_category_count.items():
    if len(categories) == 1:
        category = list(categories)[0]
        unique_words_by_category[category].append(word)

# ---------------------------
# Step 4: Show results in Streamlit
# ---------------------------
st.subheader("Unique Words per Category (Examples)")

for category, words in unique_words_by_category.items():
    st.write(f"‚úÖ **{len(words)} unique words** in category: **{category}**")
    st.write("üîπ Example words:", words[:20])


# app_top_words.py
import streamlit as st
import pandas as pd
import os
from collections import Counter
import plotly.express as px
import matplotlib.font_manager as fm

st.title("Bangla News: Top Words per Category")

# ---------------------------
# Use df_balanced from previous steps
# ---------------------------
st.subheader("Tokenized Dataset Sample")
st.dataframe(df_balanced[['category', 'token_list']].head(5))

# ---------------------------
# Compute top 100 words per category
# ---------------------------
category_top_words = {}

for category in df_balanced['category'].unique():
    tokens = df_balanced[df_balanced['category'] == category]['token_list']
    all_tokens = [token for sublist in tokens for token in sublist]
    word_freq = Counter(all_tokens)
    category_top_words[category] = word_freq.most_common(100)

# ---------------------------
# Create top_words_df
# ---------------------------
rows = []
for category, words in category_top_words.items():
    for word, freq in words:
        rows.append({'category': category, 'word': word, 'frequency': freq})

top_words_df = pd.DataFrame(rows)

st.subheader("Top Words DataFrame Sample")
st.dataframe(top_words_df.head(10))

# ---------------------------
# Bar Chart for Top 15 Words per Category
# ---------------------------
font_path = "/Users/ronjonkar/Desktop/Streamlit/NotoSansBengali-Regular.ttf"  # update if needed
fallback_font_family = "Noto Sans Bengali"

if os.path.exists(font_path):
    fm.fontManager.addfont(font_path)
    font_family = fallback_font_family
else:
    st.warning("Font file not found. Using fallback font.")
    font_family = "Nikosh"

st.subheader("Top 15 Words per Category")

for category in top_words_df['category'].unique():
    cat_df = (
        top_words_df[top_words_df['category'] == category]
        .sort_values(by='frequency', ascending=False)
        .head(15)
        .sort_values(by='frequency', ascending=True)  # horizontal plot
    )

    fig = px.bar(
        cat_df,
        x='frequency',
        y='word',
        orientation='h',
        title=f"‡¶∂‡ßç‡¶∞‡ßá‡¶£‡ßÄ: {category} - ‡¶∂‡ßÄ‡¶∞‡ßç‡¶∑ ‡ßß‡ß´ ‡¶∂‡¶¨‡ßç‡¶¶",
        labels={'frequency': 'Frequency', 'word': 'Word'},
        text='frequency',
        color='frequency',
        color_continuous_scale='Viridis'
    )

    fig.update_traces(textposition='outside')
    fig.update_layout(
        yaxis={'categoryorder': 'total ascending'},
        font=dict(family=font_family, size=16),
        coloraxis_colorbar=dict(title="‡¶ò‡¶®‡¶§‡ßç‡¶¨")
    )

    st.plotly_chart(fig, use_container_width=True)


# app_bigram.py
import streamlit as st
import pandas as pd
from collections import Counter
from itertools import tee
import plotly.express as px
import os
import matplotlib.font_manager as fm

st.title("Bangla News: Top Bigram Words per Category")

# ---------------------------
# Use df_balanced from previous steps
# ---------------------------
st.subheader("Tokenized Dataset Sample")
st.dataframe(df_balanced[['category', 'token_list']].head(5))

# ---------------------------
# Generate Bigrams
# ---------------------------
def generate_bigrams(tokens):
    a, b = tee(tokens)
    next(b, None)
    return zip(a, b)

category_bigram_freq = {}

for category in df_balanced['category'].unique():
    tokens = df_balanced[df_balanced['category'] == category]['token_list']

    all_bigrams = []
    for token_list in tokens:
        bigrams = generate_bigrams(token_list)
        all_bigrams.extend([' '.join(pair) for pair in bigrams])

    # Count bigrams
    bigram_counter = Counter(all_bigrams)
    category_bigram_freq[category] = bigram_counter.most_common(30)

# ---------------------------
# Create DataFrame for plotting
# ---------------------------
bigram_rows = []
for category, bigrams in category_bigram_freq.items():
    for bigram, freq in bigrams:
        bigram_rows.append({
            'category': category,
            'bigram': bigram,
            'frequency': freq
        })

bigram_df = pd.DataFrame(bigram_rows)

st.subheader("Top 30 Bigrams per Category")
st.dataframe(bigram_df.head(10))

# ---------------------------
# Bar Chart for Bigram
# ---------------------------
font_path = "/Users/ronjonkar/Desktop/Streamlit/NotoSansBengali-Regular.ttf"  # adjust for your system
fallback_font_family = "Noto Sans Bengali"

if os.path.exists(font_path):
    fm.fontManager.addfont(font_path)
    font_family = fallback_font_family
else:
    st.warning("Font file not found. Using fallback font.")
    font_family = "Nikosh"

st.subheader("Top 15 Bigrams Bar Charts per Category")

for category in bigram_df['category'].unique():
    cat_df = (
        bigram_df[bigram_df['category'] == category]
        .sort_values(by='frequency', ascending=False)
        .head(15)
        .sort_values(by='frequency', ascending=True)  # horizontal plot
    )

    fig = px.bar(
        cat_df,
        x='frequency',
        y='bigram',
        orientation='h',
        title=f"‡¶∂‡ßç‡¶∞‡ßá‡¶£‡ßÄ: {category} - ‡¶∂‡ßÄ‡¶∞‡ßç‡¶∑ ‡ßß‡ß´ ‡¶¨‡¶æ‡¶á‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ",
        labels={'frequency': 'Frequency', 'bigram': 'Bigram'},
        text='frequency',
        color='frequency',
        color_continuous_scale='Viridis'
    )

    fig.update_traces(textposition='outside')
    fig.update_layout(
        yaxis={'categoryorder': 'total ascending'},
        font=dict(family=font_family, size=16),
        coloraxis_colorbar=dict(title="‡¶ò‡¶®‡¶§‡ßç‡¶¨")
    )

    st.plotly_chart(fig, use_container_width=True)

# app_wordcloud.py
import streamlit as st
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import os
from collections import Counter

st.title("Bangla News: WordCloud for Unigrams")

# ---------------------------
# Use df_balanced from previous steps
# ---------------------------
st.subheader("Tokenized Dataset Sample")
st.dataframe(df_balanced[['category', 'token_list']].head(5))

# ‚úÖ Bangla font path (adjust for your system)
font_path = "/Users/ronjonkar/Desktop/Streamlit/NotoSansBengali-Regular.ttf"
if not os.path.exists(font_path):
    st.warning("Bangla font file not found. WordCloud may not render properly.")
    font_path = None  # WordCloud will use default font

# ---------------------------
# Compute top words per category if not already available
# ---------------------------
category_top_words = {}
for category in df_balanced['category'].unique():
    tokens = df_balanced[df_balanced['category'] == category]['token_list']
    all_tokens = [token for sublist in tokens for token in sublist]
    word_freq = Counter(all_tokens)
    category_top_words[category] = word_freq.most_common(100)

# ---------------------------
# Select category to display
# ---------------------------
selected_category = st.selectbox("Select Category", list(category_top_words.keys()))

# Convert list of tuples to dict for WordCloud
freq_dict = dict(category_top_words[selected_category])

# ---------------------------
# Generate WordCloud
# ---------------------------
wc = WordCloud(
    font_path=font_path,
    width=1000,
    height=500,
    background_color='white',
    max_words=100
).generate_from_frequencies(freq_dict)

# ---------------------------
# Plot WordCloud using matplotlib
# ---------------------------
fig, ax = plt.subplots(figsize=(12, 6))
ax.imshow(wc, interpolation='bilinear')
ax.axis('off')
ax.set_title(f"{selected_category} category - Top 100 Words", fontsize=16)

st.pyplot(fig)


# app_temporal.py
import streamlit as st
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import re

st.title("Bangla News Temporal Insights")

# ------------------ Bangla date parsing ------------------
BN_MONTHS = {
    "‡¶ú‡¶æ‡¶®‡ßÅ‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø": 1, "‡¶´‡ßá‡¶¨‡ßç‡¶∞‡ßÅ‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø": 2, "‡¶Æ‡¶æ‡¶∞‡ßç‡¶ö": 3, "‡¶è‡¶™‡ßç‡¶∞‡¶ø‡¶≤": 4, "‡¶Æ‡ßá": 5, "‡¶ú‡ßÅ‡¶®": 6,
    "‡¶ú‡ßÅ‡¶≤‡¶æ‡¶á": 7, "‡¶Ü‡¶ó‡¶∏‡ßç‡¶ü": 8, "‡¶∏‡ßá‡¶™‡ßç‡¶ü‡ßá‡¶Æ‡ßç‡¶¨‡¶∞": 9, "‡¶Ö‡¶ï‡ßç‡¶ü‡ßã‡¶¨‡¶∞": 10, "‡¶®‡¶≠‡ßá‡¶Æ‡ßç‡¶¨‡¶∞": 11, "‡¶°‡¶ø‡¶∏‡ßá‡¶Æ‡ßç‡¶¨‡¶∞": 12
}
BN_DIGITS = str.maketrans("‡ß¶‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ", "0123456789")

def parse_bangla_date(date_str: str):
    if not isinstance(date_str, str):
        return None
    cleaned = re.sub(r"^[^\d\u09E6-\u09EF]+", "", date_str.strip())
    cleaned = cleaned.translate(BN_DIGITS)
    pattern1 = re.compile(r"(?P<day>\d{1,2})\s+(?P<month>[^\s]+)\s+(?P<year>\d{4})(?:,\s*(?P<time>\d{1,2}:\d{2}))?")
    pattern2 = re.compile(r"(?P<time>\d{1,2}:\d{2}),\s*(?P<day>\d{1,2})\s+(?P<month>[^\s]+)\s+(?P<year>\d{4})")
    match = pattern1.search(cleaned) or pattern2.search(cleaned)
    if not match: return None
    gd = match.groupdict()
    day = int(gd["day"])
    month = BN_MONTHS.get(gd["month"], None)
    year = int(gd["year"])
    if month is None: return None
    time_part = gd.get("time") or "00:00"
    try:
        return datetime.strptime(f"{day:02d}-{month:02d}-{year} {time_part}", "%d-%m-%Y %H:%M")
    except ValueError:
        return None

# ---------------------------
# Parse Bangla dates
# ---------------------------
if 'datetime' not in df.columns:
    df['datetime'] = df['published_date'].apply(parse_bangla_date)

# ---------------------------
# Unparsed / Parsed rows
# ---------------------------
unparsed = df[df["datetime"].isna()]["published_date"].unique()
st.write(len(unparsed), "unique unparsed formats")
st.write(unparsed[:50])  # show first 50

df_parsed = df[df['datetime'].notna()].copy()
df_parsed.reset_index(drop=True, inplace=True)
st.write(f"Working with {len(df_parsed)} rows (parsed successfully).")

df_unparsed = df[df['datetime'].isna()].copy()
st.write(f"Unparsed rows are {len(df_unparsed)} (ignored for now).")

# ---------------------------
# Extract temporal features
# ---------------------------
df_parsed['year'] = df_parsed['datetime'].dt.year
df_parsed['month'] = df_parsed['datetime'].dt.month
df_parsed['day'] = df_parsed['datetime'].dt.day
df_parsed['hour'] = df_parsed['datetime'].dt.hour
df_parsed['weekday'] = df_parsed['datetime'].dt.day_name()

# ---------------------------
# 1Ô∏è‚É£ Number of articles per year
# ---------------------------
year_counts = df_parsed['year'].value_counts().sort_index()
fig, ax = plt.subplots(figsize=(8,4))
sns.barplot(x=year_counts.index, y=year_counts.values, palette="viridis", ax=ax)
ax.set_title("Articles per Year")
ax.set_ylabel("Count")
st.pyplot(fig)

# ---------------------------
# 2Ô∏è‚É£ Articles per month (overall)
# ---------------------------
month_counts = df_parsed['month'].value_counts().sort_index()
fig, ax = plt.subplots(figsize=(8,4))
sns.barplot(x=month_counts.index, y=month_counts.values, palette="magma", ax=ax)
ax.set_title("Articles per Month")
ax.set_ylabel("Count")
st.pyplot(fig)

# ---------------------------
# 3Ô∏è‚É£ Articles per weekday
# ---------------------------
weekday_counts = df_parsed['weekday'].value_counts().reindex(
    ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']
)
fig, ax = plt.subplots(figsize=(8,4))
sns.barplot(x=weekday_counts.index, y=weekday_counts.values, palette="coolwarm", ax=ax)
ax.set_title("Articles per Weekday")
ax.set_ylabel("Count")
plt.xticks(rotation=45)
st.pyplot(fig)

# ---------------------------
# 4Ô∏è‚É£ Articles per month per year (heatmap)
# ---------------------------
monthly_year_counts = df_parsed.groupby(['year','month']).size().unstack(fill_value=0)
fig, ax = plt.subplots(figsize=(12,6))
sns.heatmap(monthly_year_counts, cmap="YlGnBu", annot=True, fmt="d", ax=ax)
ax.set_title("Articles per Month per Year")
ax.set_ylabel("Year")
ax.set_xlabel("Month")
st.pyplot(fig)

# ---------------------------
# Category-wise analysis
# ---------------------------
if 'category' in df_parsed.columns:
    # 1Ô∏è‚É£ Articles per year per category
    year_cat_counts = df_parsed.groupby(['category','year']).size().unstack(fill_value=0)
    fig, ax = plt.subplots(figsize=(12,6))
    for cat in year_cat_counts.index:
        ax.plot(year_cat_counts.columns, year_cat_counts.loc[cat], marker='o', label=cat)
    ax.set_title("Articles per Year by Category")
    ax.set_xlabel("Year")
    ax.set_ylabel("Number of Articles")
    ax.legend()
    st.pyplot(fig)

    # 2Ô∏è‚É£ Articles per month per category (all years)
    month_cat_counts = df_parsed.groupby(['category','month']).size().unstack(fill_value=0)
    fig, ax = plt.subplots(figsize=(12,6))
    sns.heatmap(month_cat_counts, cmap="YlOrRd", annot=True, fmt="d", ax=ax)
    ax.set_title("Monthly Article Distribution by Category (All Years)")
    ax.set_xlabel("Month")
    ax.set_ylabel("Category")
    st.pyplot(fig)

    # 3Ô∏è‚É£ Articles per weekday per category
    weekday_order = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']
    weekday_cat_counts = df_parsed.groupby(['category','weekday']).size().unstack(fill_value=0)[weekday_order]
    fig, ax = plt.subplots(figsize=(12,6))
    sns.heatmap(weekday_cat_counts, cmap="coolwarm", annot=True, fmt="d", ax=ax)
    ax.set_title("Weekday Article Distribution by Category")
    ax.set_xlabel("Weekday")
    ax.set_ylabel("Category")
    st.pyplot(fig)

# app_embeddings.py
import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm

st.title("Bangla News: BERT Embeddings")

# ---------------------------
# Use balanced dataset
# ---------------------------
texts = df_balanced['cleaned_content'].tolist()
st.write(f"Total articles to encode (balanced dataset): {len(texts)}")

# ---------------------------
# Load Bangla BERT model (cached)
# ---------------------------
@st.cache_resource
def load_model(model_name="sagorsarker/bangla-bert-base"):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    model.eval()
    return tokenizer, model

tokenizer, model = load_model()

# ---------------------------
# Determine device
# ---------------------------
if torch.backends.mps.is_available():
    device = torch.device("mps")  # Mac GPU
elif torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

st.write(f"Using device: {device}")

# ---------------------------
# Embedding function
# ---------------------------
def get_embeddings(text_list, batch_size=32, device='cpu'):
    all_embeddings = []

    model.to(device)
    for i in tqdm(range(0, len(text_list), batch_size), desc="Encoding Batches"):
        batch_texts = text_list[i:i+batch_size]
        inputs = tokenizer(batch_texts, padding=True, truncation=True,
                           return_tensors="pt", max_length=256)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)
            # Use CLS token embedding
            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu()
            all_embeddings.append(batch_embeddings)

    all_embeddings = torch.cat(all_embeddings, dim=0)
    return all_embeddings

# ---------------------------
# Compute embeddings with progress bar
# ---------------------------
if st.button("Compute BERT Embeddings"):
    with st.spinner("Computing embeddings for balanced dataset... ‚è≥"):
        embeddings = get_embeddings(texts, batch_size=64, device=device)
    st.success("Embeddings computed ‚úÖ")
    st.write("Embeddings shape:", embeddings.shape)
